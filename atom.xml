<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>后居上个人博客</title>
  
  
  <link href="https://houjushang.com/atom.xml" rel="self"/>
  
  <link href="https://houjushang.com/"/>
  <updated>2024-11-25T02:41:10.863Z</updated>
  <id>https://houjushang.com/</id>
  
  <author>
    <name>Json Hou</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>交叉熵损失函数</title>
    <link href="https://houjushang.com/2024/11/25/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    <id>https://houjushang.com/2024/11/25/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</id>
    <published>2024-11-25T02:35:10.000Z</published>
    <updated>2024-11-25T02:41:10.863Z</updated>
    
    <content type="html"><![CDATA[<p>交叉熵损失函数（Cross-Entropy Loss）是深度学习中最常用的损失函数之一，尤其适用于分类问题。以下是对交叉熵损失函数的详细解析。</p><hr><h3 id="1-什么是交叉熵？"><a href="#1-什么是交叉熵？" class="headerlink" title="1. 什么是交叉熵？"></a>1. 什么是交叉熵？</h3><p>交叉熵来自信息论，用来衡量两个概率分布之间的差异。对于一个分类问题，它的目标是衡量模型预测的概率分布与真实类别分布之间的差距。</p><h4 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h4><p>交叉熵的数学公式如下：</p><p>CrossEntropy&#x3D;−∑i&#x3D;1Cyilog⁡(pi)\text{CrossEntropy} &#x3D; - \sum_{i&#x3D;1}^C y_i \log(p_i)</p><p>其中：</p><ul><li>CC：类别总数。</li><li>yiy_i：真实标签的独热编码（One-Hot Encoding），只有目标类别对应的 yi&#x3D;1y_i &#x3D; 1，其余 yi&#x3D;0y_i &#x3D; 0。</li><li>pip_i：模型预测的第 ii 类的概率。</li></ul><p>在多分类问题中，真实类别 yiy_i 通常用独热编码表示，这个公式可以简化为目标类别的对数概率：</p><p>CrossEntropy&#x3D;−log⁡(ptrue)\text{CrossEntropy} &#x3D; - \log(p_{\text{true}})</p><p>其中 ptruep_{\text{true}} 是模型预测的目标类别的概率。</p><hr><h3 id="2-PyTorch-中的实现"><a href="#2-PyTorch-中的实现" class="headerlink" title="2. PyTorch 中的实现"></a>2. PyTorch 中的实现</h3><h4 id="损失函数公式"><a href="#损失函数公式" class="headerlink" title="损失函数公式"></a>损失函数公式</h4><p>在 PyTorch 中，<code>nn.CrossEntropyLoss</code> 实现的交叉熵损失函数内部其实包括了两部分：</p><ol><li><strong>Softmax</strong>：将网络的输出 logits 转化为概率分布。</li><li><strong>负对数似然损失（Negative Log-Likelihood Loss, NLLLoss）</strong>：对目标类别的预测概率取负对数。</li></ol><p>公式：</p><p>Loss&#x3D;−1N∑i&#x3D;1Nlog⁡(exp⁡(zyi)∑j&#x3D;1Cexp⁡(zj))\text{Loss} &#x3D; - \frac{1}{N} \sum_{i&#x3D;1}^N \log\left(\frac{\exp(z_{y_i})}{\sum_{j&#x3D;1}^C \exp(z_j)}\right)</p><p>其中：</p><ul><li>zjz_j：模型对第 jj 类的原始输出（logit）。</li><li>yiy_i：样本 ii 的真实类别索引。</li><li>NN：批次样本数。</li><li>CC：类别总数。</li></ul><hr><h3 id="3-输入和输出"><a href="#3-输入和输出" class="headerlink" title="3. 输入和输出"></a>3. 输入和输出</h3><h4 id="输入数据要求"><a href="#输入数据要求" class="headerlink" title="输入数据要求"></a>输入数据要求</h4><ul><li><p><code>input</code></p><p>（模型预测值）：</p><ul><li>形状：(N,C,∗)(N, C, *)，其中 NN 是批次大小，CC 是类别数。</li><li>原始 logits（未经过 Softmax 处理）。</li></ul></li><li><p><code>target</code></p><p>（真实标签）：</p><ul><li>形状：(N,∗)(N, *)，其中 NN 是批次大小。</li><li>每个值是类别索引，取值范围为 [0,C−1][0, C-1]。</li></ul></li></ul><h4 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h4><ul><li>标量损失值，默认对所有样本的损失取均值。</li></ul><hr><h3 id="4-示例代码"><a href="#4-示例代码" class="headerlink" title="4. 示例代码"></a>4. 示例代码</h3><h4 id="直接使用交叉熵"><a href="#直接使用交叉熵" class="headerlink" title="直接使用交叉熵"></a>直接使用交叉熵</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型预测值 (未经过 Softmax)</span></span><br><span class="line">logits = torch.tensor([[<span class="number">2.0</span>, <span class="number">1.0</span>, <span class="number">0.1</span>], [<span class="number">1.0</span>, <span class="number">3.0</span>, <span class="number">0.1</span>]])  <span class="comment"># 2 个样本，3 个类别</span></span><br><span class="line"><span class="comment"># 真实标签</span></span><br><span class="line">labels = torch.tensor([<span class="number">0</span>, <span class="number">1</span>])  <span class="comment"># 每个样本的类别索引</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义交叉熵损失</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">loss = loss_fn(logits, labels)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Loss: <span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="自己实现-Softmax-和负对数损失"><a href="#自己实现-Softmax-和负对数损失" class="headerlink" title="自己实现 Softmax 和负对数损失"></a>自己实现 Softmax 和负对数损失</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">logits = torch.tensor([[<span class="number">2.0</span>, <span class="number">1.0</span>, <span class="number">0.1</span>], [<span class="number">1.0</span>, <span class="number">3.0</span>, <span class="number">0.1</span>]])  <span class="comment"># 2 个样本，3 个类别</span></span><br><span class="line">labels = torch.tensor([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 计算 Softmax</span></span><br><span class="line">probs = F.softmax(logits, dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Probabilities:&quot;</span>, probs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 取真实类别的概率</span></span><br><span class="line">true_probs = probs[<span class="built_in">range</span>(<span class="built_in">len</span>(labels)), labels]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;True Probabilities:&quot;</span>, true_probs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 计算负对数损失</span></span><br><span class="line">loss = -torch.log(true_probs).mean()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Loss: <span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><hr><h3 id="5-特点和注意事项"><a href="#5-特点和注意事项" class="headerlink" title="5. 特点和注意事项"></a>5. 特点和注意事项</h3><h4 id="1-不需要独热编码"><a href="#1-不需要独热编码" class="headerlink" title="(1) 不需要独热编码"></a>(1) 不需要独热编码</h4><p><code>nn.CrossEntropyLoss</code> 接受类别索引作为标签（如 <code>target = [0, 1, 2]</code>），无需独热编码。如果标签是独热编码，可以用 <code>torch.argmax</code> 转换成索引形式。</p><h4 id="2-Logits-直接输入"><a href="#2-Logits-直接输入" class="headerlink" title="(2) Logits 直接输入"></a>(2) Logits 直接输入</h4><p>不要对模型输出的 logits 再手动应用 Softmax，直接将原始输出传入 <code>nn.CrossEntropyLoss</code>。</p><h4 id="3-类别不平衡"><a href="#3-类别不平衡" class="headerlink" title="(3) 类别不平衡"></a>(3) 类别不平衡</h4><p>如果类别不平衡，可以为每个类别设置权重：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">weights = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">1.0</span>])  <span class="comment"># 假设第二类权重较高</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=weights)</span><br></pre></td></tr></table></figure><h4 id="4-高维输入"><a href="#4-高维输入" class="headerlink" title="(4) 高维输入"></a>(4) 高维输入</h4><p>对于输入形状为 (N,C,H,W)(N, C, H, W) 的数据（如图像），<code>target</code> 的形状应为 (N,H,W)(N, H, W)，并且每个像素点有一个类别索引。</p><hr><h3 id="6-应用场景"><a href="#6-应用场景" class="headerlink" title="6. 应用场景"></a>6. 应用场景</h3><h4 id="二分类"><a href="#二分类" class="headerlink" title="二分类"></a>二分类</h4><p>虽然 <code>nn.CrossEntropyLoss</code> 可以用于二分类，但更常用的是 <code>nn.BCEWithLogitsLoss</code>。如果使用交叉熵损失，类别数 CC 必须是 2。</p><h4 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h4><p>这是交叉熵损失的主要应用场景，例如手写数字识别（MNIST）、图像分类（CIFAR-10）等。</p><hr><h3 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. 总结</h3><p>交叉熵损失函数的核心是衡量模型预测概率与真实概率之间的差异，它结合了 Softmax 和对数似然损失：</p><ol><li><strong>Softmax</strong> 将模型输出转化为概率分布。</li><li><strong>负对数似然损失</strong> 聚焦于目标类别的预测概率。</li></ol><p>它是多分类任务中的标准选择，易用且高效。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;交叉熵损失函数（Cross-Entropy Loss）是深度学习中最常用的损失函数之一，尤其适用于分类问题。以下是对交叉熵损失函数的详细解析。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;1-什么是交叉熵？&quot;&gt;&lt;a href=&quot;#1-什么是交叉熵？&quot; class=&quot;headerlin</summary>
      
    
    
    
    <category term="AI学习笔记" scheme="https://houjushang.com/categories/AI%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="损失函数" scheme="https://houjushang.com/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>IDX文件格式</title>
    <link href="https://houjushang.com/2024/08/10/IDX%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F/"/>
    <id>https://houjushang.com/2024/08/10/IDX%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F/</id>
    <published>2024-08-10T11:22:08.000Z</published>
    <updated>2024-11-25T02:36:02.285Z</updated>
    
    <content type="html"><![CDATA[<p>IDX文件格式是用来存储大规模数据集的文件格式，通常用于机器学习和模式识别的训练和测试数据。特别是，MNIST手写数字数据库中的图像和标签数据就采用了IDX文件格式。</p><p>IDX文件格式有几个变种，其中<code>idx3-ubyte</code>和<code>idx1-ubyte</code>分别用于存储多维数组和一维数组的数据。具体来说：</p><ul><li><strong><code>idx3-ubyte</code></strong> 文件格式通常用于存储图像数据（多维数组），例如MNIST数据集中28x28像素的灰度图像。</li><li><strong><code>idx1-ubyte</code></strong> 文件格式通常用于存储标签数据（一维数组），例如MNIST数据集中图像对应的标签（0到9）。</li></ul><h2 id="idx3-ubyte文件格式的结构"><a href="#idx3-ubyte文件格式的结构" class="headerlink" title="idx3-ubyte文件格式的结构"></a><code>idx3-ubyte</code>文件格式的结构</h2><p><code>idx3-ubyte</code>文件包含以下部分：</p><ol><li><strong>魔数（Magic Number）</strong>：用于标识文件类型，前两字节通常为0x00和0x00，接下来的两个字节表示数据类型和维度。</li><li><strong>维度信息</strong>：表示数据的维数和各维度的大小。</li><li><strong>数据</strong>：存储实际的图像数据，以无符号字节（ubyte）形式存储。</li></ol><p><strong>文件格式</strong></p><ul><li><strong>前4字节</strong>：魔数</li><li><strong>第5到8字节</strong>：样本数量（N）</li><li><strong>第9到12字节</strong>：每个样本的行数（R）</li><li><strong>第13到16字节</strong>：每个样本的列数（C）</li><li><strong>后续字节</strong>：实际的图像数据，共 N x R x C 个字节</li></ul><h2 id="idx1-ubyte-文件结构"><a href="#idx1-ubyte-文件结构" class="headerlink" title="idx1-ubyte 文件结构"></a><code>idx1-ubyte</code> 文件结构</h2><p><code>idx1-ubyte</code> 文件包含以下部分：</p><ol><li><strong>魔数（Magic Number）</strong>：用于标识文件类型，前两字节通常为0x00和0x00，接下来的两个字节表示数据类型和维度。</li><li><strong>数据数量（Number of Items）</strong>：表示文件中包含的数据项数量。</li><li><strong>数据</strong>：实际的数据，以无符号字节（ubyte）形式存储。</li></ol><p><strong>具体细节：</strong></p><ul><li><strong>前4字节</strong>：魔数</li><li><strong>第5到8字节</strong>：数据项数量</li><li><strong>后续字节</strong>：实际的数据，每个数据项占1字节</li></ul><h2 id="魔数"><a href="#魔数" class="headerlink" title="魔数"></a>魔数</h2><p>魔数（Magic Number）来标识文件类型和数据的基本结构。魔数位于文件的前4个字节，表示数据类型和维度。以下是 IDX 文件格式中常见的魔数及其含义：</p><h3 id="常见魔数值和含义"><a href="#常见魔数值和含义" class="headerlink" title="常见魔数值和含义"></a>常见魔数值和含义</h3><p>IDX 文件的魔数由四个字节组成：</p><ol><li><strong>前两个字节</strong>：始终为 0x00 和 0x00，用于标识这是一个 IDX 文件。</li><li><strong>第三个字节</strong>：表示数据类型。</li><li><strong>第四个字节</strong>：表示数据的维数。</li></ol><h3 id="数据类型编码"><a href="#数据类型编码" class="headerlink" title="数据类型编码"></a>数据类型编码</h3><ul><li><strong>0x08</strong>：无符号字节（unsigned byte）</li><li><strong>0x09</strong>：有符号字节（signed byte）</li><li><strong>0x0B</strong>：短整型（short, 2 bytes）</li><li><strong>0x0C</strong>：整型（int, 4 bytes）</li><li><strong>0x0D</strong>：单精度浮点数（float, 4 bytes）</li><li><strong>0x0E</strong>：双精度浮点数（double, 8 bytes）</li></ul><h3 id="数据维度"><a href="#数据维度" class="headerlink" title="数据维度"></a>数据维度</h3><ul><li><strong>0x01</strong>：一维数组（vector）</li><li><strong>0x02</strong>：二维数组（matrix）</li><li><strong>0x03</strong>：三维数组（3D matrix）</li><li><strong>0x04</strong>：四维数组（4D matrix）</li></ul><h3 id="示例魔数和对应的文件类型"><a href="#示例魔数和对应的文件类型" class="headerlink" title="示例魔数和对应的文件类型"></a>示例魔数和对应的文件类型</h3><ol><li><strong>0x00000801</strong>：一维数组，每个元素为无符号字节。例如，MNIST 标签数据（idx1-ubyte）。</li><li><strong>0x00000803</strong>：三维数组，每个元素为无符号字节。例如，MNIST 图像数据（idx3-ubyte）。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;IDX文件格式是用来存储大规模数据集的文件格式，通常用于机器学习和模式识别的训练和测试数据。特别是，MNIST手写数字数据库中的图像和标签数据就采用了IDX文件格式。&lt;/p&gt;
&lt;p&gt;IDX文件格式有几个变种，其中&lt;code&gt;idx3-ubyte&lt;/code&gt;和&lt;code&gt;i</summary>
      
    
    
    
    <category term="AI学习笔记" scheme="https://houjushang.com/categories/AI%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="ai idx" scheme="https://houjushang.com/tags/ai-idx/"/>
    
  </entry>
  
</feed>
