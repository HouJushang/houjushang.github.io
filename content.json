{"meta":{"title":"后居上个人博客","subtitle":"","description":"全栈工程师尝试者","author":"Json Hou","url":"https://houjushang.com","root":"/"},"pages":[{"title":"关于","date":"2024-11-25T02:17:16.066Z","updated":"2024-11-25T02:17:16.066Z","comments":false,"path":"about/index.html","permalink":"https://houjushang.com/about/index.html","excerpt":"","text":"我就是后居上呀"},{"title":"书单","date":"2023-10-10T13:06:05.797Z","updated":"2023-10-10T13:06:05.796Z","comments":false,"path":"books/index.html","permalink":"https://houjushang.com/books/index.html","excerpt":"","text":""},{"title":"404 Not Found：该页无法显示","date":"2023-10-10T13:05:14.083Z","updated":"2023-10-10T13:05:14.083Z","comments":false,"path":"/404.html","permalink":"https://houjushang.com/404.html","excerpt":"","text":""},{"title":"友情链接","date":"2023-10-10T13:06:22.681Z","updated":"2023-10-10T13:06:22.681Z","comments":true,"path":"links/index.html","permalink":"https://houjushang.com/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2023-10-10T13:05:03.516Z","updated":"2023-10-10T13:05:03.516Z","comments":false,"path":"repository/index.html","permalink":"https://houjushang.com/repository/index.html","excerpt":"","text":""},{"title":"","date":"2023-10-10T12:55:38.106Z","updated":"2023-10-10T12:55:38.106Z","comments":true,"path":"tags/index.html","permalink":"https://houjushang.com/tags/index.html","excerpt":"","text":"sdf"}],"posts":[{"title":"交叉熵损失函数","slug":"交叉熵损失函数","date":"2024-11-25T02:35:10.000Z","updated":"2024-11-25T02:41:10.863Z","comments":true,"path":"2024/11/25/交叉熵损失函数/","link":"","permalink":"https://houjushang.com/2024/11/25/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/","excerpt":"","text":"交叉熵损失函数（Cross-Entropy Loss）是深度学习中最常用的损失函数之一，尤其适用于分类问题。以下是对交叉熵损失函数的详细解析。 1. 什么是交叉熵？交叉熵来自信息论，用来衡量两个概率分布之间的差异。对于一个分类问题，它的目标是衡量模型预测的概率分布与真实类别分布之间的差距。 公式交叉熵的数学公式如下： CrossEntropy&#x3D;−∑i&#x3D;1Cyilog⁡(pi)\\text{CrossEntropy} &#x3D; - \\sum_{i&#x3D;1}^C y_i \\log(p_i) 其中： CC：类别总数。 yiy_i：真实标签的独热编码（One-Hot Encoding），只有目标类别对应的 yi&#x3D;1y_i &#x3D; 1，其余 yi&#x3D;0y_i &#x3D; 0。 pip_i：模型预测的第 ii 类的概率。 在多分类问题中，真实类别 yiy_i 通常用独热编码表示，这个公式可以简化为目标类别的对数概率： CrossEntropy&#x3D;−log⁡(ptrue)\\text{CrossEntropy} &#x3D; - \\log(p_{\\text{true}}) 其中 ptruep_{\\text{true}} 是模型预测的目标类别的概率。 2. PyTorch 中的实现损失函数公式在 PyTorch 中，nn.CrossEntropyLoss 实现的交叉熵损失函数内部其实包括了两部分： Softmax：将网络的输出 logits 转化为概率分布。 负对数似然损失（Negative Log-Likelihood Loss, NLLLoss）：对目标类别的预测概率取负对数。 公式： Loss&#x3D;−1N∑i&#x3D;1Nlog⁡(exp⁡(zyi)∑j&#x3D;1Cexp⁡(zj))\\text{Loss} &#x3D; - \\frac{1}{N} \\sum_{i&#x3D;1}^N \\log\\left(\\frac{\\exp(z_{y_i})}{\\sum_{j&#x3D;1}^C \\exp(z_j)}\\right) 其中： zjz_j：模型对第 jj 类的原始输出（logit）。 yiy_i：样本 ii 的真实类别索引。 NN：批次样本数。 CC：类别总数。 3. 输入和输出输入数据要求 input （模型预测值）： 形状：(N,C,∗)(N, C, *)，其中 NN 是批次大小，CC 是类别数。 原始 logits（未经过 Softmax 处理）。 target （真实标签）： 形状：(N,∗)(N, *)，其中 NN 是批次大小。 每个值是类别索引，取值范围为 [0,C−1][0, C-1]。 输出 标量损失值，默认对所有样本的损失取均值。 4. 示例代码直接使用交叉熵12345678910111213import torchimport torch.nn as nn# 模型预测值 (未经过 Softmax)logits = torch.tensor([[2.0, 1.0, 0.1], [1.0, 3.0, 0.1]]) # 2 个样本，3 个类别# 真实标签labels = torch.tensor([0, 1]) # 每个样本的类别索引# 定义交叉熵损失loss_fn = nn.CrossEntropyLoss()loss = loss_fn(logits, labels)print(f&quot;Loss: &#123;loss.item()&#125;&quot;) 自己实现 Softmax 和负对数损失1234567891011121314151617import torchimport torch.nn.functional as Flogits = torch.tensor([[2.0, 1.0, 0.1], [1.0, 3.0, 0.1]]) # 2 个样本，3 个类别labels = torch.tensor([0, 1])# 1. 计算 Softmaxprobs = F.softmax(logits, dim=1)print(&quot;Probabilities:&quot;, probs)# 2. 取真实类别的概率true_probs = probs[range(len(labels)), labels]print(&quot;True Probabilities:&quot;, true_probs)# 3. 计算负对数损失loss = -torch.log(true_probs).mean()print(f&quot;Loss: &#123;loss.item()&#125;&quot;) 5. 特点和注意事项(1) 不需要独热编码nn.CrossEntropyLoss 接受类别索引作为标签（如 target = [0, 1, 2]），无需独热编码。如果标签是独热编码，可以用 torch.argmax 转换成索引形式。 (2) Logits 直接输入不要对模型输出的 logits 再手动应用 Softmax，直接将原始输出传入 nn.CrossEntropyLoss。 (3) 类别不平衡如果类别不平衡，可以为每个类别设置权重： 12weights = torch.tensor([1.0, 2.0, 1.0]) # 假设第二类权重较高loss_fn = nn.CrossEntropyLoss(weight=weights) (4) 高维输入对于输入形状为 (N,C,H,W)(N, C, H, W) 的数据（如图像），target 的形状应为 (N,H,W)(N, H, W)，并且每个像素点有一个类别索引。 6. 应用场景二分类虽然 nn.CrossEntropyLoss 可以用于二分类，但更常用的是 nn.BCEWithLogitsLoss。如果使用交叉熵损失，类别数 CC 必须是 2。 多分类这是交叉熵损失的主要应用场景，例如手写数字识别（MNIST）、图像分类（CIFAR-10）等。 7. 总结交叉熵损失函数的核心是衡量模型预测概率与真实概率之间的差异，它结合了 Softmax 和对数似然损失： Softmax 将模型输出转化为概率分布。 负对数似然损失 聚焦于目标类别的预测概率。 它是多分类任务中的标准选择，易用且高效。","categories":[{"name":"AI学习笔记","slug":"AI学习笔记","permalink":"https://houjushang.com/categories/AI%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"损失函数","slug":"损失函数","permalink":"https://houjushang.com/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"}]},{"title":"IDX文件格式","slug":"IDX文件格式","date":"2024-08-10T11:22:08.000Z","updated":"2024-11-25T02:36:02.285Z","comments":true,"path":"2024/08/10/IDX文件格式/","link":"","permalink":"https://houjushang.com/2024/08/10/IDX%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F/","excerpt":"","text":"IDX文件格式是用来存储大规模数据集的文件格式，通常用于机器学习和模式识别的训练和测试数据。特别是，MNIST手写数字数据库中的图像和标签数据就采用了IDX文件格式。 IDX文件格式有几个变种，其中idx3-ubyte和idx1-ubyte分别用于存储多维数组和一维数组的数据。具体来说： idx3-ubyte 文件格式通常用于存储图像数据（多维数组），例如MNIST数据集中28x28像素的灰度图像。 idx1-ubyte 文件格式通常用于存储标签数据（一维数组），例如MNIST数据集中图像对应的标签（0到9）。 idx3-ubyte文件格式的结构idx3-ubyte文件包含以下部分： 魔数（Magic Number）：用于标识文件类型，前两字节通常为0x00和0x00，接下来的两个字节表示数据类型和维度。 维度信息：表示数据的维数和各维度的大小。 数据：存储实际的图像数据，以无符号字节（ubyte）形式存储。 文件格式 前4字节：魔数 第5到8字节：样本数量（N） 第9到12字节：每个样本的行数（R） 第13到16字节：每个样本的列数（C） 后续字节：实际的图像数据，共 N x R x C 个字节 idx1-ubyte 文件结构idx1-ubyte 文件包含以下部分： 魔数（Magic Number）：用于标识文件类型，前两字节通常为0x00和0x00，接下来的两个字节表示数据类型和维度。 数据数量（Number of Items）：表示文件中包含的数据项数量。 数据：实际的数据，以无符号字节（ubyte）形式存储。 具体细节： 前4字节：魔数 第5到8字节：数据项数量 后续字节：实际的数据，每个数据项占1字节 魔数魔数（Magic Number）来标识文件类型和数据的基本结构。魔数位于文件的前4个字节，表示数据类型和维度。以下是 IDX 文件格式中常见的魔数及其含义： 常见魔数值和含义IDX 文件的魔数由四个字节组成： 前两个字节：始终为 0x00 和 0x00，用于标识这是一个 IDX 文件。 第三个字节：表示数据类型。 第四个字节：表示数据的维数。 数据类型编码 0x08：无符号字节（unsigned byte） 0x09：有符号字节（signed byte） 0x0B：短整型（short, 2 bytes） 0x0C：整型（int, 4 bytes） 0x0D：单精度浮点数（float, 4 bytes） 0x0E：双精度浮点数（double, 8 bytes） 数据维度 0x01：一维数组（vector） 0x02：二维数组（matrix） 0x03：三维数组（3D matrix） 0x04：四维数组（4D matrix） 示例魔数和对应的文件类型 0x00000801：一维数组，每个元素为无符号字节。例如，MNIST 标签数据（idx1-ubyte）。 0x00000803：三维数组，每个元素为无符号字节。例如，MNIST 图像数据（idx3-ubyte）。","categories":[{"name":"AI学习笔记","slug":"AI学习笔记","permalink":"https://houjushang.com/categories/AI%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"ai idx","slug":"ai-idx","permalink":"https://houjushang.com/tags/ai-idx/"}]}],"categories":[{"name":"AI学习笔记","slug":"AI学习笔记","permalink":"https://houjushang.com/categories/AI%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"损失函数","slug":"损失函数","permalink":"https://houjushang.com/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"},{"name":"ai idx","slug":"ai-idx","permalink":"https://houjushang.com/tags/ai-idx/"}]}